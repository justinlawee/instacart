# ğŸ›’ Instacart Reorder Prediction (Snowflake + dbt + ML)

This project builds a complete **AI-powered reorder prediction pipeline** on **Snowflake**, transforming raw transaction data into modeled reorder scores and surfacing them through a rich Snowsight dashboard.

It uses **dbt** for modular feature engineering, trains a **logistic regression model** locally using `scikit-learn`, and exports real predictions into Snowflake. The results are visualized interactively using native Snowflake tools â€” demonstrating what a productized ML workflow can look like inside the modern data stack.

---

## ğŸ’ªğŸ¼ What This Model Can Do
The logistic regression model predicts the probability that a user will reorder a given product in their next Instacart order. This enables a variety of high-impact use cases:

- **Personalized Recommendations**

Rank products by reorder probability to suggest top items for each userâ€™s next basket.
- **Marketing Targeting**

Identify users with high reorder intent and send timely nudges or promotions.
- **Operational Forecasting**

Use aggregate reorder probabilities to improve inventory planning and fulfillment accuracy.
- **Analytics & Dashboards**

Visualize reorder likelihood across users, products, or departments to uncover actionable patterns.

---

## ğŸ“Š Live Snowflake Dashboard

[View Dashboard (Snowsight)](https://app.snowflake.com/kctrqzo/wdb83228/#/instacart-reorder-prediction-dLPdne9M0)  
_(Snowflake login required)_

This dashboard includes:

- Model-wide KPIs (volume, avg score, % above threshold)
- Score distribution for all predictions
- Predicted reorder rate by hour of day
- Top users, products, and aisles by predicted reorder probability
- Labeled risk bands (ğŸŸ¥ğŸŸ¨ğŸŸ©) for easily surfacing low-confidence product categories

---

## âš™ï¸ Model Summary

| Metric                  | Value      |
|-------------------------|------------|
| Total Predictions       | 7,922,443  |
| Avg Predicted Score     | 0.102      |
| % Above 0.5 Threshold   | 0.6%       |

> The model was trained locally in Jupyter using `scikit-learn`, exported via `.pkl`, and used to generate predictions for all user-product pairs. These predictions were then uploaded to Snowflake and visualized through Snowsight.

---

## ğŸ§± Project Stack & Tools

| Component        | Tool Used                                |
|------------------|--------------------------------------------|
| Data Warehouse   | **Snowflake** (`INSTACART_DB`)             |
| Modeling         | **dbt** (SQL transformations)              |
| ML Training      | **scikit-learn** (logistic regression)     |
| Inference        | Local batch scoring (`.pkl` model)         |
| Dashboard        | **Snowsight** (Snowflake-native UI)        |
| App Interface    | **Streamlit** (local UI for predictions & exploration) |

---

## ğŸ§  Full AI Pipeline

- **Feature Engineering (dbt):**  
  Raw order and product data transformed into ML-ready features using reusable models

- **Model Training (Jupyter):**  
  Trained logistic regression locally on 7.9M user-product pairs  
  â†’ **Notebook**: `model_training/instacart_train_model.ipynb`

- **Model Export:**  
  Exported via `joblib` into `instacart_model.pkl`

- **Inference Results:**  
  Predictions uploaded into Snowflake as `RAW.PREDICTED_REORDERS2`

- **Dashboard View:**  
  `RAW.instacart_predictions_output` used to unify scoring logic and power dashboard

---

## ğŸ’¡ Key Insights
Most predictions score well below 0.5 â€” a calibrated model that's cautious by design

Only 0.6% of user-product pairs exceed the reorder threshold

Hour-of-day predictions peak between 10AMâ€“4PM

"At-risk" aisles like frozen foods and soft drinks receive high volume but low reorder confidence

Milk and yogurt categories show consistently high predicted reorders

---

## ğŸ“Œ How to Run This Project
Step 1: Run dbt transformations

Step 2: Train model locally using sklearn + Jupyter
         â†’ export as instacart_model.pkl

Step 3: Upload predictions to Snowflake via UI (Snowsight stage)

Step 4: Create instacart_predictions_output view

Step 5: Explore live metrics in Snowsight dashboard

---

## ğŸ—‚ï¸ Raw Dataset Source

This project uses the open-source **Instacart Online Grocery Shopping Dataset 2017**, which includes:

- Over **3 million grocery orders** from ~200,000 users  
- **50,000+ unique products** across **130 aisles**  
- Complete **userâ€“product order history**  
- Metadata for products, departments, and aisles  

The data was ingested into **Snowflake** via a user-created stage (`instacart_stage`) using standard `COPY INTO` commands.

ğŸ“¦ **Dataset URL**  
[Kaggle: Instacart Online Grocery Shopping Dataset](https://www.kaggle.com/datasets/yasserh/instacart-online-grocery-basket-analysis-dataset)

---

### âš ï¸ Large Files Not Included in This Repo

GitHub limits file uploads to 100MB, so the following large files are **excluded**:

| File                         | Approx. Size |
|------------------------------|---------------|
| `predicted_reorders.csv`     | ~250MB        |
| `order_products__prior.csv`  | ~180MB        |
| `instacart_training_input.csv` | ~260MB       | Exported from Snowflake after running dbt |

### ğŸ“¥ How to Generate `instacart_training_input.csv`

This file is **not included** in the repo due to its size (~260MB), but it's essential for model training and the Streamlit app.

To generate it:

1. Ensure your dbt environment is configured and connected to Snowflake
2. Run the model using:
```bash
dbt run --select instacart_training_input
```
3. In [Snowsight](https://app.snowflake.com), run:
```sql
SELECT * FROM RAW.instacart_training_input;
```
4. Use the download icon in Snowsight to export the result as CSV

5. Save it to:

```
original_files/instacart_training_input.csv
```

Once saved, the Streamlit app and model pipeline will work as expected.

These files are essential for full pipeline execution and should be downloaded manually from Kaggle.

---

### ğŸ“¥ Setup Instructions

1. Download and unzip the dataset from Kaggle.

2. Place the following files in your local `original_files/` directory:

```plaintext
original_files/
â”œâ”€â”€ aisles.csv
â”œâ”€â”€ departments.csv
â”œâ”€â”€ order_products__prior.csv
â”œâ”€â”€ order_products__train.csv
â”œâ”€â”€ orders.csv
â”œâ”€â”€ products.csv
â”œâ”€â”€ instacart_training_input.csv

## ğŸ”® Future Product Ideas Inspired by Project Experience

While building this project, I identified two opportunities to further enhance the Snowflake developer experience:

### 1. [Context-Aware SQL Copilot for Feature Engineering](https://docs.google.com/document/d/1Z-HQx90oDcu1zacMwie6zvGnZomQS-slF4m3_k7wrlI)
A built-in assistant within SQL worksheets that suggests and auto-generates feature engineering queries based on a projectâ€™s schema.  
Example prompts:
- *"Would you like to add a rolling 3-order average?"*
- *"Would you like to calculate reorder rates per user?"*

**Impact:**  
Accelerates ML preparation, lowers SQL barriers, and bridges data warehousing with AI workflows.

### 2. Expanded Sandbox Mode for ML Experimentation
Trial accounts currently lack access to Snowpark ML and Cortex without scheduling a sales consultation.  
Introducing a Sandbox Mode would temporarily enable these features with service limits to manage costs.

**Impact:**  
Empowers developers, students, and independent builders to explore Snowflakeâ€™s full AI/ML capabilities and complete end-to-end projects before formalizing enterprise agreements.

> **Next Step:** I have a capl scheudled a sales representative on 4/28 to explore my options for upgrading my account. I plan to expand this project further by incorporating Snowpark ML model training and Cortex GenAI functions once my account upgrade is complete.

---

## ğŸ“ Project Structure

```plaintext
instacart-reorder-prediction/
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ dashboard_preview.png                  # Snowsight dashboard screenshot
â”‚
â”œâ”€â”€ docs/                                      # Documentation and product design artifacts
â”‚   â””â”€â”€ one-pager.md                           # One-pager: Context-Aware SQL Copilot for Snowsight
â”‚
â”œâ”€â”€ models/                                    # dbt models for feature engineering & ML prep
â”‚   â”œâ”€â”€ dim_orders.sql
â”‚   â”œâ”€â”€ dim_products.sql
â”‚   â”œâ”€â”€ fct_orders.sql
â”‚   â”œâ”€â”€ fct_reorder_training_data.sql
â”‚   â”œâ”€â”€ fct_user_product_features.sql
â”‚   â”œâ”€â”€ instacart_orders.sql
â”‚   â”œâ”€â”€ instacart_predictions_output.sql
â”‚   â”œâ”€â”€ instacart_training_input.sql
â”‚   â””â”€â”€ schema.yml
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ Instacart.ipynb                        # Jupyter notebook for local model training
â”‚
â”œâ”€â”€ original_files/                            # Raw dataset CSVs from Kaggle
â”‚   â”œâ”€â”€ aisles.csv
â”‚   â”œâ”€â”€ departments.csv
â”‚   â”œâ”€â”€ order_products__train.csv
â”‚   â”œâ”€â”€ predicted_reorders.csv
â”‚   â””â”€â”€ products.csv
â”‚
â”œâ”€â”€ snowflake_sql/                             # Snowflake SQL scripts for pipeline setup
â”‚   â”œâ”€â”€ 01_ingest_instacart_data.sql           # Stage and load raw CSVs
â”‚   â”œâ”€â”€ 02_dbt_model_run.sql                   # Run dbt transformations
â”‚   â”œâ”€â”€ 03_model_upload_and_udf.sql            # (Optional) Create UDFs from model
â”‚   â”œâ”€â”€ 04_local_predictions_to_table.sql      # Upload predictions to Snowflake
â”‚   â””â”€â”€ 05_model_features_and_dummy_output.sql # Final view logic and risk labeling
â”‚
â”œâ”€â”€ snowpark/                                  # Local ML logic (Snowpark-ready structure)
â”‚   â”œâ”€â”€ local_train.py
â”‚   â””â”€â”€ model_training.py
â”‚
â”œâ”€â”€ streamlit_app.py                           # Public-facing UI for prediction browsing
â”œâ”€â”€ instacart_model.pkl                        # Exported logistic regression model
â”œâ”€â”€ dbt_project.yml                            # dbt configuration
â”œâ”€â”€ .env.example                               # Sample env config for Snowflake/Streamlit
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md                                  # Project documentation (this file)
```
---

## ğŸ™ Supporting Materials

- [One-Pager: Context-Aware SQL Copilot for Snowsight](docs/one-pager.md)

---

## ğŸ“Œ GitHub Metadata

- ğŸ§‘â€ğŸ’» Author: [Justin Borenstein-Lawee](https://www.linkedin.com/in/justin-borenstein-lawee/)  
- ğŸ•“ Last Updated: April 2025  
